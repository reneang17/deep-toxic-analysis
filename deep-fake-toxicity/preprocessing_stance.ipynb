{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic comment classification\n",
    "## Exploration and preprocessing\n",
    "\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "\n",
    "## General Outline\n",
    "\n",
    "Recall the general outline for SageMaker projects using a notebook instance.\n",
    "\n",
    "1. Unzip\n",
    "2. Process / Prepare the data.\n",
    "3. Upload the processed data to S3.\n",
    "4. Train a chosen model.\n",
    "5. Test the trained model (typically using a batch transform job).\n",
    "6. Deploy the trained model.\n",
    "7. Use the deployed model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "#paths\n",
    "import os\n",
    "\n",
    "#save\n",
    "import pickle\n",
    "\n",
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# text processing \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Inspecting for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Police find mass graves with at least '15 bodi...</td>\n",
       "      <td>712</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Body ID     Stance\n",
       "0  Police find mass graves with at least '15 bodi...      712  unrelated"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train = pd.read_csv('./data/train_stances.csv') \n",
    "\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline    0\n",
      "Body ID     0\n",
      "Stance      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# There are not null entries\n",
    "print(train.dropna().isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline    49972\n",
      "Body ID     49972\n",
      "Stance      49972\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train.dropna().notnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('./data/stance_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.- Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting comments from labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Headline', 'Body ID', 'Stance'], dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking at the reviews, one finds comments which need a lot of preprocessing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "\n",
      "Christian Bale In Talks To Play Steve Jobs In Sony's Next Biopic \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rev=115\n",
    "print('TRAIN\\n')\n",
    "print(train['Headline'][rev][:120],'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in processing the reviews is to make sure that any html tags that appear should be removed. In addition we wish to tokenize our input, that way words such as *entertained* and *entertaining* are considered the same with regard to sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "\n",
    "#rep_jumps=re.compile(r'\\n+',re.IGNORECASE)\n",
    "\n",
    "rep_special_chars=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "\n",
    "rep_special_chars= re.compile(\"[^\\w']|_\")\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "def review_to_words(review):\n",
    "    \n",
    "    \n",
    "    \n",
    "    #text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    \n",
    "    #text=rep_jumps.sub(' ', review)\n",
    "    \n",
    "    text=rep_special_chars.sub(' ', review)\n",
    "    \n",
    "    text = rep_numbers.sub('n', text) # substitute all numbers\n",
    "    \n",
    "    \n",
    "    words = text.split()[:100] # Split string into words\n",
    "    #words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    #words = [stemmer.stem(w) for w in words] # shorter words to stems \n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `review_to_words` method defined above uses `BeautifulSoup` to remove any html tags that appear and uses the `nltk` package to tokenize the reviews. As a check to ensure we know how everything is working, try applying `review_to_words` to one of the reviews in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HBO and Apple in Talks for $15/Month Apple TV Streaming Service Launching in April \n",
      " AFTER: \n",
      "\n",
      "HBO and Apple in Talks for n Month Apple TV Streaming Service Launching in April\n"
     ]
    }
   ],
   "source": [
    "rev=3\n",
    "print(train['Headline'][rev],'\\n AFTER: \\n')\n",
    "print(review_to_words(train['Headline'][rev]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below applies the `review_to_words`  to review in training and testing datasets, and caches results. This way start charge libraries and start notebook here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Headline = train.Headline.apply(review_to_words)\n",
    "train.to_json('./data/train_stance.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yet another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>ord_in_thread</th>\n",
       "      <th>author</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>crawled</th>\n",
       "      <th>site_url</th>\n",
       "      <th>country</th>\n",
       "      <th>domain_rank</th>\n",
       "      <th>thread_title</th>\n",
       "      <th>spam_score</th>\n",
       "      <th>main_img_url</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>participants_count</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "      <th>shares</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6a175f46bcd24d39b3e962ad0f29936721db70db</td>\n",
       "      <td>0</td>\n",
       "      <td>Barracuda Brigade</td>\n",
       "      <td>2016-10-26T21:41:00.000+03:00</td>\n",
       "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
       "      <td>Print They should pay all the back all the mon...</td>\n",
       "      <td>english</td>\n",
       "      <td>2016-10-27T01:49:27.168+03:00</td>\n",
       "      <td>100percentfedup.com</td>\n",
       "      <td>US</td>\n",
       "      <td>25689.0</td>\n",
       "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bias</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       uuid  ord_in_thread             author  \\\n",
       "0  6a175f46bcd24d39b3e962ad0f29936721db70db              0  Barracuda Brigade   \n",
       "\n",
       "                       published  \\\n",
       "0  2016-10-26T21:41:00.000+03:00   \n",
       "\n",
       "                                               title  \\\n",
       "0  Muslims BUSTED: They Stole Millions In Gov’t B...   \n",
       "\n",
       "                                                text language  \\\n",
       "0  Print They should pay all the back all the mon...  english   \n",
       "\n",
       "                         crawled             site_url country  domain_rank  \\\n",
       "0  2016-10-27T01:49:27.168+03:00  100percentfedup.com      US      25689.0   \n",
       "\n",
       "                                        thread_title  spam_score  \\\n",
       "0  Muslims BUSTED: They Stole Millions In Gov’t B...         0.0   \n",
       "\n",
       "                                        main_img_url  replies_count  \\\n",
       "0  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
       "\n",
       "   participants_count  likes  comments  shares  type  \n",
       "0                   1      0         0       0  bias  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./data/fake.csv') \n",
    "\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['title_text'] = train['title'] +' '+ train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Muslims BUSTED: They Stole Millions In Gov’t Benefits Print They should pay all the back all the money plus interest. The entire family and everyone who came in with them need to be deported asap. Why did it take two years to bust them? \\nHere we go again …another group stealing from the government and taxpayers! A group of Somalis stole over four million in government benefits over just 10 months! \\nWe’ve reported on numerous cases like this one where the Muslim refugees/immigrants commit fraud by scamming our system…It’s way out of control! More Related'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['title_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
